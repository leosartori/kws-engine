{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within a single batch, you must have the same number of timesteps (this is typically where you see 0-padding and masking). But between batches there is no such restriction. During inference, you can have any length.  \n",
    "(https://datascience.stackexchange.com/questions/26366/training-an-rnn-with-examples-of-different-lengths-in-keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def rnn_model(num_tokens, num_units):\n",
    "    # encoder\n",
    "    enc_input = Input(shape = (None, num_tokens))\n",
    "    enc_output, enc_state = tf.keras.layers.GRU(num_units, return_state=True)(enc_input) # input = [batch, timesteps, feature]\n",
    "    \n",
    "    # decoder\n",
    "    dec_input = Input(shape = (None, num_tokens))\n",
    "    dec_output = tf.keras.layers.GRU(num_units, return_sequences=True)(dec_input, initial_state = enc_state)\n",
    "    \n",
    "    # match input and ouput size\n",
    "    fin_output = Dense(num_tokens, activation='softmax')(dec_output)\n",
    "\n",
    "    sequence_autoencoder = Model([enc_input, dec_input], fin_output)\n",
    "    \n",
    "    encoder = Model(enc_input, enc_output)\n",
    "    \n",
    "    return sequence_autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "flatted_feature_size = 32 # number of features per sample (Mel)\n",
    "num_units = 1\n",
    "autoenc, enc = rnn_model(flatted_feature_size, num_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "10/10 [==============================] - 1s 94ms/sample - loss: 0.3158 - acc: 0.0200\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 0s 2ms/sample - loss: 0.3157 - acc: 0.0200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x6159e74248>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate random input\n",
    "num_samples = 10\n",
    "timesteps = 5\n",
    "# encoder input\n",
    "X_train = np.random.rand(num_samples, timesteps, flatted_feature_size)\n",
    "\n",
    "# decoder input, as shifted encoder input\n",
    "X_train_shifted = np.zeros(X_train.shape)\n",
    "# loop in timesteps\n",
    "for sample in range(0, X_train.shape[0]):\n",
    "    for timestep in range(0, X_train.shape[1] - 1):\n",
    "        X_train_shifted[sample, timestep + 1, :] = X_train[sample, timestep, :]\n",
    "\n",
    "# train the model\n",
    "autoenc.compile(optimizer = \"adam\", loss = tf.keras.losses.MeanSquaredError(), metrics = [\"accuracy\"])\n",
    "autoenc.fit(x = [X_train, X_train_shifted], y = X_train, epochs = 2, batch_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
